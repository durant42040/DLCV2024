# -*- coding: utf-8 -*-
"""dlcv_hw1_p1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W1eGLWQfL6CUDcVJCXq4Is-Kragkt2o1
"""


import os

import pytorch_lightning as pl
import torch
from PIL import Image
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import models, transforms

from p1.byol_pytorch.byol_pytorch import BYOL

resnet = models.resnet50(weights=None)

batch_size = 32
epochs = 100
lr = 5e-5
image_size = 128

transform = transforms.Compose(
    [
        transforms.Resize(image_size),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.RandomCrop(128, padding=4, padding_mode="reflect"),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ]
)


class ImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.image_filenames = os.listdir(image_dir)
        self.transform = transform

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_filenames[idx])
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, self.image_filenames[idx]


class SelfSupervisedLearner(pl.LightningModule):
    def __init__(self, network, **kwargs):
        super().__init__()
        self.learner = BYOL(network, **kwargs)

    def forward(self, images):
        return self.learner(images)

    def training_step(self, batch, batch_idx):
        images, _ = batch
        loss = self.forward(images)
        self.log(
            "train_loss",
            loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
            batch_size=batch_size,
        )

        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)
        return [optimizer], [scheduler]


if __name__ == "__main__":
    dataset = ImageDataset("hw1_data/p1_data/mini/train", transform=transform)

    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size

    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=7,
        persistent_workers=True,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=7,
        persistent_workers=True,
    )

    model = SelfSupervisedLearner(
        resnet,
        image_size=image_size,
        hidden_layer="avgpool",
        use_momentum=False,
        projection_size=256,
        projection_hidden_size=4096,
        moving_average_decay=0.99,
    )

    checkpoint_callback = ModelCheckpoint(
        monitor="train_loss",
        mode="min",
        filename="best-checkpoint",
        save_top_k=1,
        verbose=True,
    )

    trainer = pl.Trainer(
        max_epochs=epochs,
        accumulate_grad_batches=1,
        accelerator="gpu",
        devices="auto",
        strategy="auto",
        callbacks=[checkpoint_callback],
    )

    trainer.fit(model, train_loader, val_loader)

    trainer.save_checkpoint("pretrain/pretrain_model_SSL.ckpt")
